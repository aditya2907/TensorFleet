# ğŸš€ TensorFleet - Distributed ML Training Platform

TensorFleet is a production-ready, cloud-native distributed machine learning training platform that orchestrates ML workloads across multiple compute nodes using modern microservices architecture, gRPC communication, and Kubernetes orchestration.

## ğŸ“‹ Table of Contents

- [ğŸ—ï¸ System Architecture](#ï¸-system-architecture)
- [ğŸ”§ Service Documentation](#-service-documentation)
- [âœ¨ Platform Features](#-platform-features)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ³ Docker Development](#-docker-development)
- [â˜¸ï¸ Kubernetes Deployment](#ï¸-kubernetes-deployment)
- [ğŸ“Š Monitoring & Observability](#-monitoring--observability)
- [ğŸ§ª Testing & Validation](#-testing--validation)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸš€ Production Deployment](#-production-deployment)

## ğŸ—ï¸ System Architecture

TensorFleet implements a distributed microservices architecture optimized for machine learning workloads with horizontal scalability and fault tolerance.

```mermaid
graph TB
    subgraph "Client Layer"
        UI[React Frontend<br/>Material-UI Dashboard]
        CLI[CLI Tools<br/>Python/Go Clients]
    end
    
    subgraph "API Layer"
        GW[API Gateway<br/>Go + Gin<br/>Port 8080]
    end
    
    subgraph "Orchestration Layer"
        ORCH[Orchestrator Service<br/>Go + gRPC<br/>Port 9090]
        SCHED[Job Scheduler<br/>Task Distribution]
    end
    
    subgraph "Compute Layer"
        W1[Worker Node 1<br/>Go + gRPC<br/>Port 9091]
        W2[Worker Node 2<br/>Go + gRPC<br/>Port 9092] 
        W3[Worker Node N<br/>Go + gRPC<br/>Port 909N]
        MLW[ML Worker<br/>Python + Flask<br/>Port 8085]
    end
    
    subgraph "Storage & Registry Layer"
        MODELS[Model Service<br/>Python + Flask<br/>Port 8084]
        STORAGE[Storage Service<br/>Python + Flask<br/>Port 8082]
        MINIO[MinIO S3<br/>Object Storage<br/>Port 9000]
        MONGO[MongoDB<br/>GridFS + Collections<br/>Port 27017]
    end
    
    subgraph "Observability Layer"
        MON[Monitoring Service<br/>Python + Flask<br/>Port 8081]
        PROM[Prometheus<br/>Metrics Collection<br/>Port 9090]
        GRAF[Grafana<br/>Visualization<br/>Port 3000]
    end
    
    subgraph "Infrastructure Layer"
        REDIS[Redis<br/>Caching & Queues<br/>Port 6379]
        NGINX[Nginx<br/>Load Balancer<br/>Port 80/443]
    end

    %% Client connections
    UI --> GW
    CLI --> GW
    
    %% API Gateway routing
    GW --> ORCH
    GW --> MODELS
    GW --> STORAGE
    GW --> MON
    
    %% Orchestration
    ORCH --> W1
    ORCH --> W2
    ORCH --> W3
    ORCH --> MLW
    SCHED --> ORCH
    
    %% Storage connections
    MODELS --> MONGO
    STORAGE --> MINIO
    MLW --> MONGO
    W1 --> REDIS
    W2 --> REDIS
    W3 --> REDIS
    
    %% Monitoring connections
    W1 --> PROM
    W2 --> PROM
    W3 --> PROM
    MLW --> PROM
    MON --> PROM
    GRAF --> PROM
    
    %% Load balancing
    NGINX --> GW
    
    style UI fill:#e1f5fe
    style GW fill:#fff3e0
    style ORCH fill:#f3e5f5
    style W1 fill:#e8f5e8
    style W2 fill:#e8f5e8
    style W3 fill:#e8f5e8
    style MLW fill:#e8f5e8
    style MODELS fill:#fff8e1
    style STORAGE fill:#fff8e1
    style MON fill:#fce4ec
```

### ğŸ¯ Core Architectural Principles

- **Microservices Architecture**: Each service is independently deployable and scalable
- **Event-Driven Communication**: Asynchronous processing with gRPC and REST APIs
- **Cloud-Native Design**: Kubernetes-first deployment with service mesh capabilities
- **Observability-First**: Comprehensive monitoring, logging, and distributed tracing
- **Fault Tolerance**: Circuit breakers, retries, and graceful degradation
- **Security by Design**: API authentication, TLS encryption, and RBAC controls

## ğŸ”§ Service Documentation

Each TensorFleet service is documented with comprehensive setup, API, and deployment guides:

### Core Services

| Service | Technology | Port | Documentation | Purpose |
|---------|------------|------|---------------|---------|
| **API Gateway** | Go + Gin | 8080 | [ğŸ“– README](./api-gateway/README.md) | HTTP/REST entry point, request routing, authentication |
| **Orchestrator** | Go + gRPC | 9090 | [ğŸ“– README](./orchestrator/README.md) | Job scheduling, task distribution, worker management |
| **Worker** | Go + gRPC | 9091+ | [ğŸ“– README](./worker/README.md) | Distributed compute nodes, training execution |
| **ML Worker** | Python + Flask | 8085 | [ğŸ“– README](./worker-ml/README.md) | Machine learning training engine with MongoDB |

### Data & Storage Services

| Service | Technology | Port | Documentation | Purpose |
|---------|------------|------|---------------|---------|
| **Model Service** | Python + Flask | 8084 | [ğŸ“– README](./model-service/README.md) | Model registry, versioning, GridFS storage |
| **Storage Service** | Python + Flask | 8082 | [ğŸ“– README](./storage/README.md) | S3-compatible object storage, dataset management |

### Platform Services

| Service | Technology | Port | Documentation | Purpose |
|---------|------------|------|---------------|---------|
| **Frontend** | React + Vite | 3000 | [ğŸ“– README](./frontend/README.md) | Web dashboard, real-time monitoring UI |
| **Monitoring** | Python + Flask | 8081 | [ğŸ“– README](./monitoring/README.md) | Metrics aggregation, health monitoring, analytics |

### Infrastructure Dependencies

| Component | Technology | Port | Purpose |
|-----------|------------|------|---------|
| **MongoDB** | Document DB | 27017 | Model metadata, GridFS file storage |
| **Redis** | In-Memory DB | 6379 | Caching, job queues, session storage |
| **MinIO** | Object Storage | 9000 | S3-compatible file storage for datasets |
| **Prometheus** | Monitoring | 9090 | Metrics collection and alerting |
| **Grafana** | Visualization | 3000 | Metrics dashboards and analytics |

## âœ¨ Platform Features

### ğŸ¯ Core ML Platform Capabilities

- **ğŸš€ Distributed Training**: Automatically scale ML training across multiple worker nodes
- **ğŸ§  Model Registry**: Version-controlled model storage with metadata and GridFS integration
- **ğŸ“Š Real-time Monitoring**: Live training metrics, system health, and performance dashboards
- **ğŸ”„ Job Orchestration**: Intelligent task scheduling with load balancing and fault tolerance
- **ğŸ—„ï¸ Data Management**: S3-compatible storage for datasets, models, and artifacts
- **ğŸ”’ Security & Auth**: JWT authentication, RBAC, and secure service-to-service communication

### ğŸ› ï¸ Developer Experience

- **ğŸ“¡ RESTful APIs**: Comprehensive REST endpoints for all platform interactions
- **ğŸ”Œ gRPC Services**: High-performance inter-service communication
- **ğŸ³ Docker-First**: Container-native development and deployment
- **â˜¸ï¸ Kubernetes-Ready**: Production-grade orchestration with Helm charts
- **ğŸ“ˆ Observability**: Prometheus metrics, distributed tracing, and structured logging
- **ğŸ§ª Testing Suite**: Comprehensive unit, integration, and load testing

### ğŸ¨ Frontend Dashboard Features

- **ğŸ“Š Real-time Analytics**: Live job status, worker utilization, and training progress
- **ğŸ›ï¸ Job Management**: Submit, monitor, and manage ML training jobs
- **ğŸ“ˆ Metrics Visualization**: Interactive charts and graphs for training metrics
- **ğŸ–¥ï¸ System Health**: Service status monitoring and health checks
- **ğŸ‘¤ User Management**: Role-based access control and user authentication
- **ğŸ”” Notifications**: Real-time alerts and job completion notifications

## âš¡ Quick Start

### ğŸš€ One-Line Setup

Get TensorFleet running in under 2 minutes with Docker Compose:

```bash
# Clone repository
git clone https://github.com/your-org/tensorfleet.git
cd tensorfleet

# Start all services
docker-compose up -d

# Verify deployment
make status-check

# Access the dashboard
open http://localhost:3000
```

### ğŸ¯ Quick Demo

Run a complete ML training workflow:

```bash
# Submit a training job via API
curl -X POST http://localhost:8080/api/v1/jobs \
  -H "Content-Type: application/json" \
  -d '{
    "algorithm": "random_forest",
    "dataset": "iris",
    "hyperparameters": {
      "n_estimators": 100,
      "max_depth": 5
    }
  }'

# Monitor job progress
curl http://localhost:8080/api/v1/jobs/{job_id}/status

# View training metrics
open http://localhost:3000/jobs/{job_id}
```

### ğŸ“± Access Points

| Service | URL | Credentials |
|---------|-----|-------------|
| **Frontend Dashboard** | http://localhost:3000 | - |
| **API Gateway** | http://localhost:8080 | - |
| **Model Registry** | http://localhost:8084/api/v1/models | - |
| **Grafana Metrics** | http://localhost:3001 | admin/admin |
| **MinIO Console** | http://localhost:9001 | admin/password123 |

## ğŸ³ Docker Development

### ğŸƒ Development Mode

Start services in development mode with hot reloading:

```bash
# Start core infrastructure
docker-compose -f docker-compose.development.yml up -d mongodb redis minio

# Start services in development mode
make dev-start

# View logs
make logs

# Run specific service
docker-compose -f docker-compose.development.yml up api-gateway
```

### ğŸ”§ Service Management

```bash
# Start all services
make start

# Stop all services  
make stop

# Restart specific service
make restart SERVICE=api-gateway

# View service status
make status

# Clean up resources
make cleanup
```

### ğŸ› ï¸ Development Utilities

```bash
# Install dependencies
make install-deps

# Run tests
make test

# Build all images
make build

# Format code
make format

# Run linting
make lint
```

## â˜¸ï¸ Kubernetes Deployment

### ğŸš€ Production Deployment

Deploy TensorFleet to a Kubernetes cluster:

```bash
# Create namespace
kubectl apply -f k8s/namespace.yaml

# Deploy infrastructure
kubectl apply -f k8s/infrastructure.yaml

# Deploy core services
kubectl apply -f k8s/deployment.yaml

# Setup ingress
kubectl apply -f k8s/ingress.yaml

# Verify deployment
kubectl get pods -n tensorfleet
```

### ğŸ“Š Scaling Configuration

```yaml
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-hpa
  namespace: tensorfleet
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: worker
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### ğŸ” Security & Secrets

```bash
# Create TLS certificates
kubectl create secret tls tensorfleet-tls \
  --cert=path/to/tls.crt \
  --key=path/to/tls.key \
  -n tensorfleet

# Create database credentials
kubectl create secret generic mongodb-secret \
  --from-literal=connection-string="mongodb://admin:password123@mongodb:27017/tensorfleet?authSource=admin" \
  -n tensorfleet

# Create API keys
kubectl create secret generic api-keys \
  --from-literal=jwt-secret="your-jwt-secret" \
  --from-literal=admin-key="your-admin-api-key" \
  -n tensorfleet
```

## ğŸ“Š Monitoring & Observability

### ğŸ¯ Key Metrics

TensorFleet provides comprehensive observability across all layers:

#### Application Metrics
- **Job Metrics**: Success rate, completion time, queue depth
- **Training Metrics**: Accuracy, loss, convergence rate, resource utilization
- **Worker Metrics**: Task throughput, error rate, resource consumption
- **API Metrics**: Request latency, error rate, throughput by endpoint

#### Infrastructure Metrics
- **Service Health**: Availability, response time, resource usage
- **Database Performance**: Connection pool, query performance, storage usage
- **Storage Metrics**: Object storage usage, transfer rates, availability
- **Network Metrics**: Service-to-service communication, latency, errors

### ğŸ“ˆ Grafana Dashboards

Pre-configured dashboards available at `http://localhost:3001`:

- **TensorFleet Overview**: High-level platform metrics and health
- **Job Analytics**: Training job performance and success rates
- **Worker Performance**: Distributed worker utilization and efficiency
- **Infrastructure Health**: System resources and service availability
- **API Performance**: Gateway metrics and endpoint analytics

### ğŸ”” Alerting Rules

```yaml
# Critical system alerts
groups:
  - name: tensorfleet-critical
    rules:
    - alert: ServiceDown
      expr: up{job="tensorfleet"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "TensorFleet service {{ $labels.instance }} is down"
    
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High error rate on {{ $labels.service }}"
```

## ğŸ§ª Testing & Validation

### ğŸ” Test Suite Overview

TensorFleet includes comprehensive testing at multiple levels:

```bash
# Run all tests
make test-all

# Unit tests
make test-unit

# Integration tests  
make test-integration

# API tests
make test-api

# Load tests
make test-load

# Security tests
make test-security
```

### ğŸš€ Continuous Integration

```yaml
# GitHub Actions workflow
name: TensorFleet CI/CD
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Run Tests
      run: |
        docker-compose -f docker-compose.test.yml up --abort-on-container-exit
    - name: Build Images
      run: make build
    - name: Security Scan
      run: make security-scan
```

### ğŸ“Š Test Coverage

| Service | Unit Tests | Integration Tests | API Tests |
|---------|------------|-------------------|-----------|
| API Gateway | 95%+ | âœ… | âœ… |
| Orchestrator | 90%+ | âœ… | âœ… |
| Worker | 88%+ | âœ… | âœ… |
| Model Service | 92%+ | âœ… | âœ… |
| Storage | 89%+ | âœ… | âœ… |
| Monitoring | 87%+ | âœ… | âœ… |
| Frontend | 85%+ | âœ… | âœ… |

## ğŸ“ Project Structure

TensorFleet follows a microservices architecture with clear separation of concerns:

```
TensorFleet/
â”œâ”€â”€ ğŸŒ api-gateway/          # HTTP/REST API Gateway (Go + Gin)
â”‚   â”œâ”€â”€ main.go              # Gateway server implementation  
â”‚   â”œâ”€â”€ handlers/            # HTTP request handlers
â”‚   â”œâ”€â”€ middleware/          # Authentication, CORS, logging
â”‚   â”œâ”€â”€ go.mod               # Go dependencies
â”‚   â””â”€â”€ Dockerfile           # Container configuration
â”‚
â”œâ”€â”€ ğŸ¯ orchestrator/         # Job Orchestration Service (Go + gRPC)
â”‚   â”œâ”€â”€ main.go              # Orchestrator server
â”‚   â”œâ”€â”€ scheduler/           # Task scheduling logic
â”‚   â”œâ”€â”€ worker_manager/      # Worker registration & health
â”‚   â”œâ”€â”€ go.mod               # Go dependencies  
â”‚   â””â”€â”€ Dockerfile           # Container configuration
â”‚
â”œâ”€â”€ âš¡ worker/               # Distributed Worker Nodes (Go + gRPC)
â”‚   â”œâ”€â”€ main.go              # Worker server implementation
â”‚   â”œâ”€â”€ executor/            # Task execution engine
â”‚   â”œâ”€â”€ metrics/             # Performance monitoring
â”‚   â”œâ”€â”€ go.mod               # Go dependencies
â”‚   â””â”€â”€ Dockerfile           # Container configuration
â”‚
â”œâ”€â”€ ğŸ¤– worker-ml/            # ML Training Engine (Python + Flask)
â”‚   â”œâ”€â”€ main.py              # ML worker API server
â”‚   â”œâ”€â”€ models/              # ML algorithm implementations
â”‚   â”œâ”€â”€ datasets/            # Dataset loaders and preprocessors
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ Dockerfile           # Container configuration
â”‚
â”œâ”€â”€ ğŸ—„ï¸ model-service/        # Model Registry (Python + Flask + MongoDB)
â”‚   â”œâ”€â”€ main.py              # Model management API
â”‚   â”œâ”€â”€ storage/             # GridFS integration
â”‚   â”œâ”€â”€ metadata/            # Model metadata handling
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ Dockerfile           # Container configuration
â”‚
â”œâ”€â”€ ğŸ“¦ storage/              # Object Storage Service (Python + Flask + MinIO)
â”‚   â”œâ”€â”€ main.py              # Storage API server
â”‚   â”œâ”€â”€ storage_manager.py   # MinIO client wrapper
â”‚   â”œâ”€â”€ handlers/            # File upload/download logic
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ Dockerfile           # Container configuration
â”‚
â”œâ”€â”€ ğŸ“Š monitoring/           # Observability Service (Python + Flask)
â”‚   â”œâ”€â”€ main.py              # Monitoring API server  
â”‚   â”œâ”€â”€ collectors/          # Metrics collection
â”‚   â”œâ”€â”€ aggregators/         # Data aggregation logic
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ Dockerfile           # Container configuration
â”‚
â”œâ”€â”€ ğŸ¨ frontend/             # Web Dashboard (React + Vite + Material-UI)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/      # React components
â”‚   â”‚   â”œâ”€â”€ pages/           # Application pages
â”‚   â”‚   â”œâ”€â”€ hooks/           # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ services/        # API client services
â”‚   â”‚   â””â”€â”€ utils/           # Utility functions
â”‚   â”œâ”€â”€ public/              # Static assets
â”‚   â”œâ”€â”€ package.json         # Node.js dependencies
â”‚   â”œâ”€â”€ vite.config.js       # Vite configuration
â”‚   â””â”€â”€ Dockerfile           # Container configuration
â”‚
â”œâ”€â”€ ğŸ”Œ proto/                # gRPC Protocol Definitions
â”‚   â”œâ”€â”€ gateway.proto        # API Gateway service definitions
â”‚   â”œâ”€â”€ orchestrator.proto   # Orchestrator service definitions  
â”‚   â”œâ”€â”€ worker.proto         # Worker service definitions
â”‚   â””â”€â”€ generate.sh          # Protocol buffer generation script
â”‚
â”œâ”€â”€ â˜¸ï¸ k8s/                  # Kubernetes Deployment Manifests
â”‚   â”œâ”€â”€ namespace.yaml       # TensorFleet namespace
â”‚   â”œâ”€â”€ infrastructure.yaml  # MongoDB, Redis, MinIO
â”‚   â”œâ”€â”€ configmap.yaml       # Configuration management
â”‚   â”œâ”€â”€ deployment.yaml      # Core service deployments
â”‚   â”œâ”€â”€ ingress.yaml         # External access configuration
â”‚   â”œâ”€â”€ monitoring.yaml      # Prometheus & Grafana
â”‚   â””â”€â”€ storage.yaml         # Persistent volume claims
â”‚
â”œâ”€â”€ ğŸ§ª tests/                # Comprehensive Testing Suite
â”‚   â”œâ”€â”€ unit/                # Unit tests for each service
â”‚   â”œâ”€â”€ integration/         # Integration tests  
â”‚   â”œâ”€â”€ api/                 # API endpoint tests
â”‚   â”œâ”€â”€ load/                # Load testing scripts
â”‚   â””â”€â”€ e2e/                 # End-to-end testing
â”‚
â”œâ”€â”€ ğŸ“œ scripts/              # Automation & Utility Scripts
â”‚   â”œâ”€â”€ demo-*.sh            # Demo and testing scripts
â”‚   â”œâ”€â”€ cleanup-*.sh         # Environment cleanup utilities
â”‚   â”œâ”€â”€ setup-*.sh           # Environment setup scripts
â”‚   â””â”€â”€ test-*.sh            # Testing automation
â”‚
â”œâ”€â”€ ğŸ³ Docker Configurations
â”‚   â”œâ”€â”€ docker-compose.yml           # Production deployment
â”‚   â”œâ”€â”€ docker-compose.development.yml # Development environment
â”‚   â””â”€â”€ docker-compose.test.yml      # Testing environment
â”‚
â”œâ”€â”€ ğŸ“‹ Configuration Files
â”‚   â”œâ”€â”€ Makefile             # Build automation and common tasks
â”‚   â”œâ”€â”€ .env.example         # Environment variable template
â”‚   â”œâ”€â”€ .gitignore           # Git ignore patterns
â”‚   â”œâ”€â”€ netlify.toml         # Frontend deployment config
â”‚   â””â”€â”€ vercel.json          # Alternative frontend deployment
â”‚
â””â”€â”€ ğŸ“š Documentation
    â”œâ”€â”€ README.md            # Main project documentation (this file)
    â”œâ”€â”€ docs/                # Additional documentation
    â””â”€â”€ postman/             # API testing collections
```

### ğŸ—ï¸ Architecture Highlights

- **ğŸ”„ Service Communication**: gRPC for internal services, REST for external APIs
- **ğŸ“Š Data Flow**: MongoDB â†’ GridFS â†’ Model Registry â†’ API Gateway â†’ Frontend  
- **ğŸš€ Scalability**: Horizontal pod autoscaling for workers and compute nodes
- **ğŸ”’ Security**: JWT authentication, TLS encryption, network policies
- **ğŸ“ˆ Observability**: Prometheus metrics, Grafana dashboards, structured logging
- **ğŸ³ Deployment**: Container-native with Kubernetes orchestration

## ğŸš€ Production Deployment

### ğŸŒŠ Cloud Platform Support

TensorFleet supports deployment on major cloud platforms:

#### Amazon Web Services (AWS)
```bash
# Deploy to EKS
eksctl create cluster --name tensorfleet-production --region us-west-2
kubectl apply -f k8s/

# Configure ALB Ingress
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.4/docs/install/iam_policy.json
```

#### Google Cloud Platform (GCP)
```bash  
# Deploy to GKE
gcloud container clusters create tensorfleet-production --zone us-central1-a
kubectl apply -f k8s/

# Configure Cloud Load Balancer
kubectl apply -f k8s/ingress-gcp.yaml
```

#### Microsoft Azure
```bash
# Deploy to AKS  
az aks create --resource-group tensorfleet-rg --name tensorfleet-production
kubectl apply -f k8s/

# Configure Azure Load Balancer
kubectl apply -f k8s/ingress-azure.yaml
```

### ğŸ”§ Production Configuration

#### Environment Variables
```bash
# Production environment configuration
export ENVIRONMENT=production
export LOG_LEVEL=INFO
export MONGODB_URL=mongodb+srv://prod:password@cluster.mongodb.net/tensorfleet
export REDIS_URL=redis://prod-redis.tensorfleet.svc.cluster.local:6379
export MINIO_ENDPOINT=s3.amazonaws.com
export JWT_SECRET=your-production-jwt-secret
export API_RATE_LIMIT=1000
export WORKER_MAX_REPLICAS=100
```

#### Resource Limits
```yaml
# Production resource configuration
resources:
  requests:
    memory: "512Mi"
    cpu: "250m"
  limits:
    memory: "2Gi" 
    cpu: "1000m"
```

#### High Availability Setup
```yaml
# Multi-zone deployment
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - tensorfleet-worker
      topologyKey: "kubernetes.io/zone"
```

### ğŸ” Security Hardening

#### Network Policies
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: tensorfleet-network-policy
spec:
  podSelector:
    matchLabels:
      app: tensorfleet
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: tensorfleet
    ports:
    - protocol: TCP
      port: 8080
```

#### RBAC Configuration
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: tensorfleet-worker
rules:
- apiGroups: [""]
  resources: ["pods", "services"]
  verbs: ["get", "list", "watch"]
```

### ğŸ“Š Production Monitoring

#### Prometheus Configuration
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'tensorfleet'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
```

#### Grafana Dashboards
- **TensorFleet Overview**: High-level system metrics
- **Training Jobs**: ML job performance and progress
- **Infrastructure**: Kubernetes cluster health  
- **Application Performance**: Service response times and errors

### ğŸš¨ Alerting Rules

```yaml
groups:
  - name: tensorfleet-production
    rules:
    - alert: TensorFleetServiceDown
      expr: up{job="tensorfleet"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "TensorFleet service is down"
        description: "Service {{ $labels.instance }} has been down for more than 5 minutes"
    
    - alert: HighJobFailureRate  
      expr: rate(tensorfleet_jobs_failed_total[10m]) > 0.1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High job failure rate detected"
```

### ğŸ”„ Disaster Recovery

#### Backup Strategy
```bash
# Automated MongoDB backup
kubectl create cronjob mongodb-backup \
  --image=mongo:5.0 \
  --schedule="0 2 * * *" \
  -- mongodump --uri="$MONGODB_URI" --out=/backup/$(date +%Y%m%d)

# MinIO backup to S3
kubectl create cronjob minio-backup \
  --image=minio/mc \
  --schedule="0 3 * * *" \
  -- mc mirror local-minio s3-backup/tensorfleet-backup
```

#### Recovery Procedures
```bash
# Restore MongoDB from backup
kubectl exec -it mongodb-pod -- mongorestore --uri="$MONGODB_URI" /backup/20241208

# Restore MinIO from S3 backup  
kubectl exec -it minio-pod -- mc mirror s3-backup/tensorfleet-backup local-minio
```

### âš¡ Performance Optimization

#### Database Optimization
```javascript
// MongoDB indexes for production workloads
db.jobs.createIndex({ "status": 1, "created_at": -1 })
db.models.createIndex({ "algorithm": 1, "metrics.accuracy": -1 })
db.workers.createIndex({ "status": 1, "last_heartbeat": -1 })
```

#### Caching Strategy  
```yaml
# Redis configuration for production
redis:
  maxmemory: 2gb
  maxmemory-policy: allkeys-lru
  save: "900 1 300 10 60 10000"
```

#### Auto-scaling Configuration
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tensorfleet-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tensorfleet-worker
  minReplicas: 5
  maxReplicas: 100
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## ğŸ¤ Contributing

### Development Workflow

1. **Fork & Clone**
   ```bash
   git clone https://github.com/your-username/tensorfleet.git
   cd tensorfleet
   ```

2. **Create Feature Branch**
   ```bash
   git checkout -b feature/amazing-new-feature
   ```

3. **Development Setup**
   ```bash
   # Install development dependencies
   make install-dev
   
   # Start development environment
   make dev-start
   
   # Run tests
   make test
   ```

4. **Code Quality**
   ```bash
   # Format code
   make format
   
   # Run linting
   make lint
   
   # Security scan
   make security-check
   ```

5. **Submit Changes**
   ```bash
   git add .
   git commit -m "feat: add amazing new feature"
   git push origin feature/amazing-new-feature
   ```

### Code Standards

- **Go**: Follow `gofmt`, `golint`, and `go vet` standards
- **Python**: PEP 8 compliance with `black` formatting
- **JavaScript**: ESLint with Prettier formatting  
- **Documentation**: Clear comments and comprehensive README updates
- **Testing**: Minimum 85% code coverage for new features

### Pull Request Process

1. Ensure all tests pass and code coverage meets requirements
2. Update documentation for any new features or API changes
3. Add integration tests for new endpoints or services
4. Request review from maintainers
5. Address feedback and maintain clean commit history

## ğŸ“ Support & Community

### Getting Help

- **ğŸ“– Documentation**: Comprehensive guides in each service directory
- **ğŸ› Bug Reports**: [GitHub Issues](https://github.com/your-org/tensorfleet/issues)
- **ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/your-org/tensorfleet/discussions)  
- **ğŸ“§ Email**: support@tensorfleet.io

### Community Resources

- **ğŸŒŸ Demo Videos**: [YouTube Playlist](https://youtube.com/tensorfleet)
- **ğŸ“ Blog Posts**: [Medium Publication](https://medium.com/tensorfleet)
- **ğŸ¤ Conference Talks**: [Speaker Deck](https://speakerdeck.com/tensorfleet)
- **ğŸ’¼ LinkedIn**: [TensorFleet Company Page](https://linkedin.com/company/tensorfleet)

### Enterprise Support

For production deployments and enterprise features:
- **ğŸ¢ Enterprise Consulting**: Custom deployment assistance
- **ğŸ”’ Security Audits**: Professional security assessments  
- **ğŸ“ˆ Performance Tuning**: Optimization for large-scale workloads
- **ğŸ“ Training Programs**: Team training and certification

---

## ğŸ“„ License

TensorFleet is released under the [MIT License](LICENSE). See the LICENSE file for full terms.

## ğŸ™ Acknowledgments

Built with â¤ï¸ by the TensorFleet team using:

- **Languages**: Go, Python, JavaScript/TypeScript
- **Frameworks**: Gin (Go), Flask (Python), React (JavaScript)
- **Infrastructure**: Kubernetes, Docker, gRPC, MongoDB, Redis, MinIO
- **Monitoring**: Prometheus, Grafana, OpenTelemetry
- **Cloud Platforms**: AWS, GCP, Azure support

---

**â­ Star this repository if TensorFleet helps your ML workflows!**
- âœ… **Task Queuing** - Orchestrator manages task distribution
- âœ… **Auto-scaling** - Kubernetes HPA for worker nodes
- âœ… **Object Storage** - MinIO for models, datasets, and checkpoints
- âœ… **Real-time Metrics** - Prometheus + Grafana monitoring
- âœ… **Health Checks** - Liveness and readiness probes
- âœ… **Horizontal Scaling** - Workers scale from 2-10 pods

### Production Features
- ğŸ”’ **Secure Defaults** - No hardcoded credentials
- ğŸ“Š **Observability** - Structured logging, metrics, traces
- ğŸ³ **Containerized** - Docker images for all services
- â˜¸ï¸ **Kubernetes-native** - Complete K8s manifests
- ğŸ”„ **High Availability** - Stateful sets for infrastructure
- ğŸ¯ **Load Balancing** - Service discovery and routing

## ğŸ“¦ Prerequisites

### Local Development
- Docker 20.10+
- Docker Compose 2.0+
- Go 1.21+ (for proto generation)
- Node.js 16+ (for frontend development)

### Kubernetes Deployment
- Kubernetes cluster 1.24+
- kubectl configured
- Helm 3.0+ (optional, for Prometheus/Grafana)
- Container registry access (Docker Hub, GitHub Container Registry, etc.)

## ï¿½ Project Reproducibility Instructions

### System Requirements
Ensure your system meets the following requirements for consistent reproduction:

**Hardware:**
- Minimum: 4 GB RAM, 2 CPU cores
- Recommended: 8 GB RAM, 4 CPU cores
- Disk Space: 5 GB free space

**Operating System:**
- macOS 10.15+ / Ubuntu 18.04+ / Windows 10+ with WSL2
- Docker Desktop or Docker Engine installed and running

### Step-by-Step Reproduction Guide

#### 1. Environment Setup

```bash
# Clone the repository
git clone https://github.com/your-username/TensorFleet.git
cd TensorFleet

# Verify Docker installation
docker --version
docker-compose --version

# Ensure Docker is running
docker ps
```

#### 2. Build Protocol Buffers (Optional)

```bash
# Install Protocol Buffer compiler (if modifying .proto files)
# macOS
brew install protobuf protoc-gen-go protoc-gen-go-grpc

# Ubuntu
sudo apt-get install -y protobuf-compiler
go install google.golang.org/protobuf/cmd/protoc-gen-go@latest
go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest

# Generate proto files (only if modified)
cd proto && ./generate.sh && cd ..
```

#### 3. Initial Setup & Validation

```bash
# Pull all required Docker images
docker-compose pull

# Build all services (this may take 5-10 minutes on first run)
docker-compose build

# Verify all images are built
docker images | grep tensorfleet
```

#### 4. Start the Platform

```bash
# Start all services
docker-compose up -d

# Wait for all services to be healthy (30-60 seconds)
# You should see all services as "healthy"
docker-compose ps

# Verify health status
curl http://localhost:8080/health  # API Gateway
curl http://localhost:8081/health  # Storage Service  
curl http://localhost:8082/health  # Monitoring Service
```

#### 5. Smoke Test - Verify Everything Works

```bash
# Run the automated demo (tests all endpoints)
./quick-api-demo.sh

# Expected output should show:
# âœ“ All services healthy
# âœ“ Storage operations working
# âœ“ Job submission and monitoring working
# âœ“ Metrics collection working
```

#### 6. Access Web Interfaces

Open these URLs in your browser:
- **Frontend Dashboard**: http://localhost:3000
- **Grafana Monitoring**: http://localhost:3001 (admin/admin)
- **Prometheus Metrics**: http://localhost:9090
- **MinIO Console**: http://localhost:9001 (minioadmin/minioadmin)

### Troubleshooting Common Issues

#### Issue: Services fail to start
```bash
# Check logs for specific service
docker-compose logs <service-name>

# Common solutions:
# 1. Restart Docker Desktop
# 2. Clear Docker cache: docker system prune -a
# 3. Check port conflicts: lsof -i :8080
```

#### Issue: "Connection refused" errors
```bash
# Wait for health checks to pass
watch docker-compose ps

# Services need 30-60 seconds to fully initialize
# Redis and MinIO must be healthy before other services start
```

#### Issue: Port conflicts
```bash
# Check what's using the ports
lsof -i :3000 -i :8080 -i :8081 -i :8082 -i :9000 -i :9001

# Stop conflicting services or modify docker-compose.yml ports
```

#### Issue: Out of disk space
```bash
# Clean up Docker resources
docker system prune -a --volumes

# Remove old containers and images
docker container prune
docker image prune -a
```

### Reproducible Demo Scenarios

#### Scenario 1: Submit and Monitor Training Job
```bash
# Submit a ResNet50 training job
curl -X POST http://localhost:8080/api/v1/jobs \
  -H "Content-Type: application/json" \
  -d '{
    "model_type": "resnet50",
    "dataset_path": "s3://tensorfleet/datasets/imagenet",
    "hyperparameters": {"learning_rate": "0.001"},
    "num_workers": 3,
    "epochs": 10
  }'

# Monitor progress in real-time
# Save the job_id from above response, then:
watch -n 2 "curl -s http://localhost:8080/api/v1/jobs/YOUR_JOB_ID | jq"
```

#### Scenario 2: Upload and Download Files
```bash
# Create a sample dataset
echo "epoch,loss,accuracy" > sample_dataset.csv
echo "1,2.5,0.3" >> sample_dataset.csv
echo "2,1.8,0.5" >> sample_dataset.csv

# Upload to storage
curl -X POST http://localhost:8081/api/v1/upload/datasets/sample.csv \
  -F "file=@sample_dataset.csv"

# List files
curl http://localhost:8081/api/v1/list/datasets | jq

# Download file
curl http://localhost:8081/api/v1/download/datasets/sample.csv
```

#### Scenario 3: View Monitoring Dashboard
1. Open http://localhost:3001 in browser
2. Login with admin/admin
3. Navigate to "TensorFleet Dashboard"
4. Submit some jobs and watch metrics update

### Environment Variables for Customization

Create a `.env` file to customize settings:
```bash
# Optional: Customize ports
API_GATEWAY_PORT=8080
STORAGE_PORT=8081
MONITORING_PORT=8082
FRONTEND_PORT=3000

# Optional: Customize MinIO credentials
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=password123

# Optional: Worker scaling
WORKER_REPLICAS=3
```

### Data Persistence

All data is persisted in Docker volumes:
```bash
# View volumes
docker volume ls | grep tensorfleet

# To reset all data (WARNING: destroys all jobs/files)
docker-compose down -v

# To backup data
docker run --rm -v tensorfleet_minio_data:/data -v $(pwd):/backup ubuntu tar czf /backup/minio_backup.tar.gz /data
```

### Cleanup Instructions

```bash
# Stop all services
docker-compose down

# Remove all data (optional)
docker-compose down -v

# Clean up Docker resources
docker system prune -a

# Remove built images
docker rmi $(docker images | grep tensorfleet | awk '{print $3}')
```

### Testing Reproducibility

To verify the setup works on a clean system:
```bash
# Test script for CI/CD or clean environment
#!/bin/bash
set -e

echo "Testing TensorFleet reproducibility..."
docker-compose up -d
sleep 60  # Wait for services to initialize

# Test health endpoints
curl -f http://localhost:8080/health
curl -f http://localhost:8081/health  
curl -f http://localhost:8082/health

# Test job submission
JOB_ID=$(curl -s -X POST http://localhost:8080/api/v1/jobs \
  -H "Content-Type: application/json" \
  -d '{"model_type":"test","dataset_path":"test","num_workers":1,"epochs":1}' | jq -r .job_id)

# Verify job was created
curl -f "http://localhost:8080/api/v1/jobs/$JOB_ID"

echo "âœ… Reproducibility test passed!"
```

## ï¿½ğŸš€ Quick Start

### 1. Clone and Setup

```bash
git clone <repository-url>
cd TensorFleet
```

### 2. Generate gRPC Stubs

```bash
make proto
```

### 3. Run Locally with Docker Compose

```bash
# Build and start all services
make compose-up

# Or manually:
docker-compose up --build
```

### 4. Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Frontend | http://localhost:3000 | - |
| API Gateway | http://localhost:8080 | - |
| Storage API | http://localhost:8081 | - |
| Monitoring API | http://localhost:8082 | - |
| Model Service API | http://localhost:8083 | - |
| ML Worker API | http://localhost:8000 | - |
| Grafana | http://localhost:3001 | admin/admin |
| Prometheus | http://localhost:9090 | - |
| MinIO Console | http://localhost:9001 | minioadmin/minioadmin |
| MongoDB | localhost:27017 | admin/password123 |

## ğŸ’» Local Development

### Start Services

```bash
# Start all services in background
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
make compose-down
```

### Submit a Training Job

```bash
# Using curl
curl -X POST http://localhost:8080/api/v1/jobs \
  -H "Content-Type: application/json" \
  -H "X-User-ID: demo-user" \
  -d '{
    "model_type": "cnn",
    "dataset_path": "/data/mnist",
    "num_workers": 3,
    "epochs": 10,
    "hyperparameters": {
      "learning_rate": "0.001",
      "batch_size": "64"
    }
  }'

# Response
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "RUNNING",
  "num_tasks": 100,
  "message": "Job created with 100 tasks"
}
```

### Check Job Status

```bash
curl http://localhost:8080/api/v1/jobs/550e8400-e29b-41d4-a716-446655440000
```

### View Metrics

```bash
# Dashboard metrics
curl http://localhost:8082/api/v1/dashboard

# Prometheus metrics
curl http://localhost:8082/metrics
```

## ğŸ¤– MongoDB ML Training

TensorFleet now supports machine learning training with MongoDB for dataset storage and model persistence.

### Quick ML Training Demo

```bash
# Run the automated ML training demo
./demo-mongodb-ml.sh
```

This demo will:
1. âœ… Train 3 different ML models (RandomForest, LogisticRegression, SVM)
2. âœ… Store trained models in MongoDB using GridFS
3. âœ… Save model metadata (hyperparameters, metrics, version)
4. âœ… Download and save models locally
5. âœ… Display model statistics

### Manual ML Training

#### 1. List Available Datasets

```bash
curl http://localhost:8000/datasets | jq
```

**Response:**
```json
{
  "datasets": [
    {
      "name": "iris",
      "description": "Iris flower dataset",
      "n_samples": 150,
      "n_features": 4,
      "target_column": "species"
    },
    {
      "name": "wine",
      "description": "Wine classification dataset",
      "n_samples": 178,
      "n_features": 13,
      "target_column": "wine_class"
    }
  ]
}
```

#### 2. Train a Model

```bash
curl -X POST http://localhost:8000/train \
  -H "Content-Type: application/json" \
  -d '{
    "job_id": "my_training_job_001",
    "dataset_name": "iris",
    "algorithm": "random_forest",
    "target_column": "species",
    "model_name": "iris_rf_model",
    "hyperparameters": {
      "n_estimators": 100,
      "max_depth": 5,
      "random_state": 42
    }
  }' | jq
```

**Response:**
```json
{
  "job_id": "my_training_job_001",
  "model_id": "507f1f77bcf86cd799439011",
  "status": "completed",
  "metrics": {
    "train_accuracy": 0.9833,
    "test_accuracy": 0.9667,
    "training_time": 0.234
  },
  "model_name": "iris_rf_model",
  "version": "v1701964800"
}
```

#### 3. List Trained Models

```bash
curl "http://localhost:8083/api/v1/models?page=1&limit=10" | jq
```

**Response:**
```json
{
  "models": [
    {
      "id": "507f1f77bcf86cd799439011",
      "name": "iris_rf_model",
      "algorithm": "random_forest",
      "metrics": {
        "test_accuracy": 0.9667,
        "train_accuracy": 0.9833
      },
      "version": "v1701964800",
      "created_at": "2025-12-07T10:30:00Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 10,
    "total": 1,
    "pages": 1
  }
}
```

#### 4. Get Model Metadata

```bash
curl http://localhost:8083/api/v1/models/<model_id> | jq
```

**Response:**
```json
{
  "id": "507f1f77bcf86cd799439011",
  "name": "iris_rf_model",
  "algorithm": "random_forest",
  "hyperparameters": {
    "n_estimators": 100,
    "max_depth": 5,
    "random_state": 42
  },
  "metrics": {
    "train_accuracy": 0.9833,
    "test_accuracy": 0.9667,
    "training_time": 0.234
  },
  "version": "v1701964800",
  "dataset_name": "iris",
  "target_column": "species",
  "features": ["sepal length", "sepal width", "petal length", "petal width"],
  "created_at": "2025-12-07T10:30:00Z"
}
```

#### 5. Download a Model

```bash
# Download model file
curl http://localhost:8083/api/v1/models/<model_id>/download \
  -o my_model.pkl

# Verify download
ls -lh my_model.pkl
```

#### 6. Use Downloaded Model (Python)

```python
import pickle
import numpy as np

# Load the model
with open('my_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Make predictions
sample_data = np.array([[5.1, 3.5, 1.4, 0.2]])
prediction = model.predict(sample_data)
print(f"Prediction: {prediction}")
```

### Supported ML Algorithms

| Algorithm | Description | Best For |
|-----------|-------------|----------|
| `random_forest` | Random Forest Classifier | Classification, robust to overfitting |
| `logistic_regression` | Logistic Regression | Binary/multi-class classification |
| `svm` | Support Vector Machine | Non-linear classification |
| `decision_tree` | Decision Tree Classifier | Interpretable models |

### Model Hyperparameters

**Random Forest:**
```json
{
  "n_estimators": 100,
  "max_depth": null,
  "min_samples_split": 2,
  "random_state": 42
}
```

**Logistic Regression:**
```json
{
  "max_iter": 1000,
  "C": 1.0,
  "random_state": 42
}
```

**SVM:**
```json
{
  "kernel": "rbf",
  "C": 1.0,
  "gamma": "scale",
  "random_state": 42
}
```

**Decision Tree:**
```json
{
  "max_depth": null,
  "min_samples_split": 2,
  "random_state": 42
}
```

### Python Client for ML Operations

Use the provided Python client for easy interaction:

```bash
# Run complete demo
python ml_client.py demo

# List all models
python ml_client.py list

# Get statistics
python ml_client.py stats

# Download a specific model
python ml_client.py download <model_id> output.pkl
```

### Model Storage Architecture

- **Datasets**: Stored in MongoDB collections or GridFS for large files
- **Trained Models**: Serialized with pickle and stored in GridFS
- **Metadata**: Model information stored in MongoDB collections
  - Model name, algorithm, hyperparameters
  - Training metrics (accuracy, loss)
  - Version information
  - Dataset reference
  - Feature names
  - Training timestamp

### Model Versioning

Each trained model is automatically versioned using a timestamp:
- Format: `v{unix_timestamp}`
- Example: `v1701964800`
- Allows multiple versions of the same model
- Easy rollback to previous versions

## â˜¸ï¸ Kubernetes Deployment

### 1. Build and Push Images

```bash
# Set your registry
export REGISTRY=ghcr.io/your-username/tensorfleet

# Build all images
make build

# Push to registry
make push
```

### 2. Update Image References

Edit `k8s/*.yaml` files and replace `ghcr.io/your-username/tensorfleet` with your actual registry.

### 3. Deploy to Kubernetes

```bash
# Deploy everything
make k8s-deploy

# Or step-by-step:
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/infrastructure.yaml
kubectl apply -f k8s/orchestrator.yaml
kubectl apply -f k8s/worker.yaml
kubectl apply -f k8s/api-gateway.yaml
kubectl apply -f k8s/storage.yaml
kubectl apply -f k8s/monitoring.yaml
kubectl apply -f k8s/frontend.yaml
kubectl apply -f k8s/ingress.yaml
```

### 4. Verify Deployment

```bash
# Check all pods
kubectl get pods -n tensorfleet

# Check services
kubectl get svc -n tensorfleet

# View logs
make logs

# Or specific service:
kubectl logs -n tensorfleet -l app=orchestrator -f
```

### 5. Access Services

```bash
# Get API Gateway external IP
kubectl get svc -n tensorfleet api-gateway-service

# Get Frontend external IP
kubectl get svc -n tensorfleet frontend-service

# Port-forward for local access
kubectl port-forward -n tensorfleet svc/api-gateway-service 8080:80
kubectl port-forward -n tensorfleet svc/frontend-service 3000:80
```

## ğŸ“š API Documentation

### Job Submission

**POST** `/api/v1/jobs`

Request:
```json
{
  "model_type": "cnn",
  "dataset_path": "/data/mnist",
  "num_workers": 2,
  "epochs": 10,
  "hyperparameters": {
    "learning_rate": "0.001",
    "batch_size": "64"
  }
}
```

Response:
```json
{
  "job_id": "uuid",
  "status": "RUNNING",
  "num_tasks": 50,
  "message": "Job created successfully"
}
```

### Job Status

**GET** `/api/v1/jobs/:id`

Response:
```json
{
  "job_id": "uuid",
  "status": "RUNNING",
  "progress": 45,
  "completed_tasks": 23,
  "total_tasks": 50,
  "current_loss": 0.234,
  "current_accuracy": 0.897
}
```

### Storage API

**POST** `/api/v1/upload/:bucket/:path`
- Upload file to storage

**GET** `/api/v1/download/:bucket/:path`
- Download file from storage

**GET** `/api/v1/list/:bucket`
- List all objects in bucket

### MongoDB ML APIs

#### Train Model
**POST** `/train` (ML Worker - Port 8000)

Request:
```json
{
  "job_id": "unique_job_id",
  "dataset_name": "iris",
  "algorithm": "random_forest",
  "target_column": "species",
  "model_name": "my_model",
  "hyperparameters": {
    "n_estimators": 100,
    "max_depth": 5
  }
}
```

Response:
```json
{
  "job_id": "unique_job_id",
  "model_id": "507f1f77bcf86cd799439011",
  "status": "completed",
  "metrics": {
    "train_accuracy": 0.98,
    "test_accuracy": 0.96
  }
}
```

#### List Models
**GET** `/api/v1/models?page=1&limit=20&algorithm=random_forest` (Model Service - Port 8083)

Response:
```json
{
  "models": [...],
  "pagination": {
    "page": 1,
    "limit": 20,
    "total": 45,
    "pages": 3
  }
}
```

#### Get Model Metadata
**GET** `/api/v1/models/:model_id` (Model Service - Port 8083)

#### Download Model
**GET** `/api/v1/models/:model_id/download` (Model Service - Port 8083)

Returns: Binary model file (pickle format)

#### Delete Model
**DELETE** `/api/v1/models/:model_id` (Model Service - Port 8083)

#### Get Statistics
**GET** `/api/v1/statistics` (Model Service - Port 8083)

Response:
```json
{
  "total_models": 45,
  "algorithm_stats": [
    {
      "_id": "random_forest",
      "count": 20,
      "avg_accuracy": 0.95
    }
  ],
  "recent_models": [...]
}
```

## ğŸ“Š Monitoring

### Prometheus Metrics

Worker metrics (`:2112/metrics`):
- `worker_task_duration_seconds` - Task execution time
- `worker_tasks_completed_total` - Completed tasks counter
- `worker_tasks_failed_total` - Failed tasks counter

Monitoring service metrics (`:8082/metrics`):
- `tensorfleet_job_submissions_total` - Total job submissions
- `tensorfleet_active_jobs` - Currently active jobs
- `tensorfleet_active_workers` - Active worker nodes
- `tensorfleet_training_loss` - Training loss by job_id
- `tensorfleet_training_accuracy` - Training accuracy by job_id

### Grafana Dashboards

Access Grafana at `http://localhost:3001` (admin/admin) to view:
- System overview dashboard
- Worker node metrics
- Training progress visualization

## ğŸ“ Project Structure

```
tensorfleet/
â”œâ”€â”€ api-gateway/          # REST API gateway (Go)
â”‚   â”œâ”€â”€ main.go
â”‚   â”œâ”€â”€ go.mod
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ proto/           # Generated gRPC stubs
â”œâ”€â”€ orchestrator/         # Job orchestrator (Go)
â”‚   â”œâ”€â”€ main.go
â”‚   â”œâ”€â”€ go.mod
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ proto/
â”œâ”€â”€ worker/               # Training worker (Go)
â”‚   â”œâ”€â”€ main.go
â”‚   â”œâ”€â”€ go.mod
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ proto/
â”œâ”€â”€ worker-ml/            # ML training worker (Python)
â”‚   â”œâ”€â”€ main.py          # ML training logic
â”‚   â”œâ”€â”€ api_server.py    # Flask API server
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ model-service/        # Model management service (Python)
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ storage/              # Object storage service (Python)
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ monitoring/           # Metrics service (Python)
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/             # Web UI (React)
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ proto/                # Protocol buffer definitions
â”‚   â”œâ”€â”€ gateway.proto
â”‚   â”œâ”€â”€ orchestrator.proto
â”‚   â”œâ”€â”€ worker.proto
â”‚   â””â”€â”€ generate.sh
â”œâ”€â”€ k8s/                  # Kubernetes manifests
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”œâ”€â”€ infrastructure.yaml
â”‚   â”œâ”€â”€ mongodb-ml.yaml
â”‚   â”œâ”€â”€ api-gateway.yaml
â”‚   â”œâ”€â”€ orchestrator.yaml
â”‚   â”œâ”€â”€ worker.yaml
â”‚   â”œâ”€â”€ storage.yaml
â”‚   â”œâ”€â”€ monitoring.yaml
â”‚   â”œâ”€â”€ frontend.yaml
â”‚   â””â”€â”€ ingress.yaml
â”œâ”€â”€ ml_client.py         # Python client for ML operations
â”œâ”€â”€ demo-mongodb-ml.sh   # MongoDB ML training demo
â”œâ”€â”€ docker-compose.yml   # Local development
â”œâ”€â”€ Makefile             # Build automation
â””â”€â”€ README.md            # This file
```

## ğŸ› ï¸ Development

### Running Individual Services

```bash
# API Gateway
cd api-gateway && go run main.go

# Orchestrator
cd orchestrator && go run main.go

# Worker
cd worker && go run main.go

# Storage
cd storage && python main.py

# Monitoring
cd monitoring && python main.py

# Frontend
cd frontend && npm start
```

### Testing

```bash
# Unit tests (Go services)
cd api-gateway && go test ./...
cd orchestrator && go test ./...
cd worker && go test ./...

# Integration tests
./scripts/integration-test.sh
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ï¿½ Additional Documentation

- **[MongoDB ML Setup Guide](MONGODB_ML_GUIDE.md)** - Complete setup and deployment guide
- **[MongoDB ML Summary](MONGODB_ML_SUMMARY.md)** - Summary of MongoDB integration
- **[Quick Reference](QUICK_REFERENCE.md)** - Command reference card
- **[Demo README](DEMO_README.md)** - API demo scripts documentation

## ï¿½ğŸ“ Support

For issues and questions:
- GitHub Issues: [repository-url]/issues
- Documentation: [repository-url]/wiki

---

**Built with â¤ï¸ using Go, Python, gRPC, MongoDB, and modern DevOps practices**

